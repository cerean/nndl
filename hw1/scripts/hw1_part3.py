# -*- coding: utf-8 -*-
"""HW1_part3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aqPilwt_aZPbUyFieZH9O8aFRbVWyU8-

# CS 84020 Neural Networks and Deep Learning
#### Homework 1
#### Andrea Ceres and Shao Liu

# **PART3**

# Listing 8
#### Load libraries. Load the Wisconsin Diagnostic Breast Cancer dataset. Complete any 8 calculations and plottings using Seaborn package which are not included into the previous calculations and plottings with matplotlib. Two of these might be related to PCA.
1 Bivariate Density Plot

2) Categorized Empirical Cumulative Distribution Plot

3) Empirical Cumulative Distribution Plot

4) PCA Plot

5) Violin Plot

6) Strip Plot

7) Box Plot

8) Feature Selection, Prediction and Confusion Matrix

**Analysis**
From the Bivariate density plot of the first 10 features, we can see that only two feature can somehow reflect the difference between two labels(green and pink). For example, ‘compactness_mean’ and ‘perimeter_mean’ density plot shows two highly separated distributions of two labels (green and pink). However, ‘compactness_mean’ and ‘smoothness_mean’ formed same distribution of two labels which means it’s hard to distinguish two class by these two features. Categorized empirical cumulative distribution also shows different distribution of two labels. ‘perimeter_mean’ has extremely different distribution between two labels. Empirical cumulative distribution plot shows the unique distribution of each feature. PCA(2 components) shows how well the best two features can do to classify the categories. From the plot, we can see that blue spots and orange spots are nearly separated by two features in a 2 dimension space. 

The correlation matrix from Part 2: Listing 6, violin plots, strip plots, and bar plot below shed light on which features are least correlated to the diagnosis and to which there is little variation between the benign and malignant interquartiles. This, in addition to the dependent features ('perimeter_\*' and 'area_\*') described earlier, informs us in the dropping of some features in feature selection: 'fractal_dimension_mean', 'texture_se', 'smoothness_se', 'symmetry_se', 'fractal_dimension_se'. Rerunning the model afterwards yields improved performance for every model, except linear discriminant analysis and naive bayes. For our purpose of diagnosing malignancies with the least amount of misses, the decision tree classifier performs the best, yielding a recall of 97% for the malignant class.

a) Load libraries.
"""

# Load libraries
from pandas import read_csv, set_option
from pandas.plotting import scatter_matrix
from matplotlib import pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from google.colab import data_table
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn import decomposition

# Set options
set_option('display.max_columns', 32)
# plt.style.use('seaborn-talk')
plt.style.use('seaborn-white')

"""b) Load the dataset."""

# Load dataset
filename = 'http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'
colnames = ['id', 'diagnosis', 
         'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 
         'smoothness_mean', 'compactness_mean', 'concavity_mean', 
         'concave_points_mean', 'symmetry_mean', 'fractal_dimension_mean', 
         'radius_se', 'texture_se', 'perimeter_se', 'area_se', 
         'smoothness_se', 'compactness_se', 'concavity_se', 
         'concave_points_se', 'symmetry_se', 'fractal_dimension_se', 
         'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 
         'smoothness_worst', 'compactness_worst', 'concavity_worst', 
         'concave_points_worst', 'symmetry_worst', 'fractal_dimension_worst']
dataset = read_csv(filename, names=colnames, header=None)

# drop `id` identifier
dataset = dataset.drop(['id'], 1)

# recode target labels to 0 and 1
dataset['diagnosis'] = dataset['diagnosis'].map({'B':0, 'M':1})

# Standardization
Y = dataset.iloc[:,0]
X_orig = dataset.iloc[:,1:]
X = (X_orig - X_orig.mean()) / (X_orig.std())

# X_orig = dataset.values[:, 1:]
# Y = dataset.values[:, 0]
# scaler_standardized = StandardScaler()
# X = scaler_standardized.fit_transform(X_orig)
X

"""c) Data visualization.

1）Bivariate Density Plot (first 10 features)
"""

#maximum absolute scaling
df_scaled = dataset.copy()

for column in df_scaled.columns[1:32]:
    df_scaled[column] = df_scaled[column]  / df_scaled[column].abs().max()
df_scaled

features = list(dataset.columns[0:11])
data = df_scaled[features]

g = sns.PairGrid(data, hue='diagnosis', palette = 'PiYG_r')
# g.map(sns.scatterplot)
g.map_upper(sns.scatterplot)
g.map_lower(sns.kdeplot)
g.map_diag(sns.kdeplot, lw=3, legend=False)

"""2) Categorized Empirical Cumulative Distribution Plot"""

# g2 = sns.PairGrid(data, hue='diagnosis', palette = 'PiYG_r')
g2 = sns.PairGrid(data, hue='diagnosis', palette = 'PiYG_r')
g2.map_diag(sns.ecdfplot, lw=3, legend=False)
g2.map_offdiag(sns.scatterplot)

"""3) Empirical Cumulative Distribution Plot"""

features1 = list(dataset.columns[1:11])
data1 = df_scaled[features1]
plt.figure(figsize=(15, 10))
sns.ecdfplot(data1)

features2 = list(dataset.columns[11:21])
data2 = df_scaled[features2]
plt.figure(figsize=(15, 10))
sns.ecdfplot(data2)

features3 = list(dataset.columns[21:31])
data3 = df_scaled[features3]
plt.figure(figsize=(15, 10))
sns.ecdfplot(data3)

"""4) PCA plot (2 components)"""

pca = decomposition.PCA(n_components=2)
pc = pca.fit_transform(X)
pc_df = pd.DataFrame(data = pc , 
        columns = ['PC1', 'PC2'])
pc_df['Cluster'] = df_scaled['diagnosis']
sns.lmplot( x="PC1", y="PC2",
  data=pc_df, 
  fit_reg=False, 
  hue='Cluster', # color by cluster
  legend=True,
  )

"""5) Violin Plot"""

data1_10 = pd.concat([Y, X.iloc[:,0:10]],axis=1)
data1_10 = pd.melt(data1_10, id_vars="diagnosis",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(25,10))
sns.violinplot(x="features", y="value", hue="diagnosis", data=data1_10, split=True, 
               inner="quartile", palette={0: "mistyrose", 1: "hotpink"})
plt.xticks(rotation=90)

data11_20 = pd.concat([Y, X.iloc[:,10:20]],axis=1)
data11_20 = pd.melt(data11_20, id_vars="diagnosis",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(25,10))
sns.violinplot(x="features", y="value", hue="diagnosis", data=data11_20, split=True, 
               inner="quartile", palette={0: "mistyrose", 1: "hotpink"})
plt.xticks(rotation=90)

data21_30 = pd.concat([Y, X.iloc[:,20:]],axis=1)
data21_30 = pd.melt(data21_30, id_vars="diagnosis",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(25,10))
sns.violinplot(x="features", y="value", hue="diagnosis", data=data21_30, split=True, 
               inner="quartile", palette={0: "mistyrose", 1: "hotpink"})
plt.xticks(rotation=90)

"""6) Strip Plot"""

sns.set(style = 'dark')
sns.stripplot(x='features', y='value', data=data1_10, hue='diagnosis', size=1.5)
plt.xticks(rotation=90)
plt.show()

sns.stripplot(x='features', y='value', data=data11_20, hue='diagnosis', size=1.5)
plt.xticks(rotation=90)
plt.show()

sns.stripplot(x='features', y='value', data=data21_30, hue='diagnosis', size=1.5)
plt.xticks(rotation=90)
plt.show()

"""7) Box Plot"""

bp = sns.boxplot(x='diagnosis', y='texture_se', data=dataset, showfliers=False, palette='PRGn_r')
bp.set_title('Example of a feature with little correlation to diagnosis: texture_se')
plt.show()

"""8) Feature Selection and Confusion Matrix"""

# Based on dependence determined, drop perimeter and area features.
# Based on the correlation matrix and violin plots, drop features that have very little correlation to the target.
drop_list = ['perimeter_mean', 'area_mean', 'perimeter_se', 'area_se', 'perimeter_worst', 'area_worst', 
'fractal_dimension_mean', 'texture_se', 'smoothness_se', 'symmetry_se', 'fractal_dimension_se']

X_fs = X.drop(drop_list,axis=1)
X_fs.head()

val_size = 0.20
seed = 7
X_train, X_val, Y_train, Y_val = train_test_split(X_fs, Y, test_size=val_size, random_state=seed)

def model_predict(name, model):
    model.fit(X_train, Y_train)
    pred = model.predict(X_val)
    print("%s: %f" % (name, accuracy_score(Y_val, pred)))
    cm = confusion_matrix(Y_val, pred)
    sns.heatmap(cm, annot=True, fmt="d")
    print(classification_report(Y_val, pred))
    
    return pred

pred_lr = model_predict('LR', LogisticRegression(solver='liblinear'))

pred_lda = model_predict('LDA', LinearDiscriminantAnalysis())

pred_knn = model_predict('KNN', KNeighborsClassifier())

pred_cart = model_predict('CART', DecisionTreeClassifier(random_state=seed))

pred_nb = model_predict('NB', GaussianNB())

pred_svm = model_predict('SVM', SVC())