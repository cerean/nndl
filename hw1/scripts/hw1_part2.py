# -*- coding: utf-8 -*-
"""HW1_part2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XaRE_tIuKglKEqE_KD-xF3F7os9ldWVW

# CS 84020 Neural Networks and Deep Learning
#### Homework 1
#### Andrea Ceres and Shao Liu

# **PART2**

# Listing 6
#### Load libraries. Load the Wisconsin Diagnostic Breast Cancer dataset. Data Visualization.
**Analysis**
The list of pairwise Pearson correlation of data shows that 'perimeter_mean' and 'radius_mean' have the largest correlation among all the 30 attributes. From the correlation matrix, we conclude that the 'perimeter', 'radius' and 'area' are highly correlated because the mean, se and worst value of these three attributes have have large correlation. On the other hand, 'texture' is less related to all of the three. 'fractal_dimension', 'texture', 'symmetry' and ‘smoothness’ are four features that have the least correlation with other features. The skew of data shows that 'area_se' has the most positive skew. This means 'area_se' has some extreme larger data. We can also conclude this from the density plot: For 'area_se', the tail on the right is longer than the tail on the left, because there are a few variables that have large values, which makes the tail on the right side of the curve longer.

a) Load libraries.
"""

# Load libraries
from pandas import read_csv, set_option
from pandas.plotting import scatter_matrix
from matplotlib import pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from google.colab import data_table
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer, Binarizer

# Set options
set_option('display.max_columns', 32)

# plt.style.use('seaborn-talk')
plt.style.use('seaborn-white')

"""b) Load the dataset."""

# Load dataset
filename = 'http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'
colnames = ['id', 'diagnosis', 
         'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 
         'smoothness_mean', 'compactness_mean', 'concavity_mean', 
         'concave_points_mean', 'symmetry_mean', 'fractal_dimension_mean', 
         'radius_se', 'texture_se', 'perimeter_se', 'area_se', 
         'smoothness_se', 'compactness_se', 'concavity_se', 
         'concave_points_se', 'symmetry_se', 'fractal_dimension_se', 
         'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 
         'smoothness_worst', 'compactness_worst', 'concavity_worst', 
         'concave_points_worst', 'symmetry_worst', 'fractal_dimension_worst']
dataset = read_csv(filename, names=colnames, header=None)

# drop `id` identifier
dataset = dataset.drop(['id'], 1)

# recode target labels to 0 and 1
dataset['diagnosis'] = dataset['diagnosis'].map({'B':0, 'M':1})
dataset.head(3)

"""c) Data visualization.

Pairwise Pearson Correlation
"""

features_pearson = list(dataset.columns[1:32])
dataset[features_pearson].corr(method='pearson')

pd.set_option('display.max_rows', 1000)

sorted_mat = dataset[features_pearson].corr(method='pearson').unstack().sort_values(ascending=False) 

# Print the 10 strongest positive pairwise correlations after self-correlations.
print('* Strongest Positive Correlations *\n', sorted_mat[31:].head(10))

# Print the 10 weakest pairwise correlations.
print('\n* Closest to Zero Correlations *\n', sorted_mat[:807].tail(10))

# Print the 10 strongest negative pairwise correlations.
print('\n* Strongest Negative Correlations *\n', sorted_mat.tail(10))

"""Skew for Each Attribute"""

features = list(dataset.columns[1:32])
s = dataset[features].skew()
plt.figure(figsize=(19, 15))
s.plot(kind = 'bar', fontsize=15, color='hotpink')
plt.title('Skew for Each Attribute', fontsize=16, pad=20);

"""Univariate Density Plot"""

dataset.select_dtypes('float').plot(kind='kde', subplots=True, layout=(20,5), figsize=(25,35), 
             sharex=False, sharey=False, fontsize=5, color='hotpink')
plt.tight_layout()
plt.show()

"""Correlation Matrix Plot"""

f = plt.figure(figsize=(19, 15))
features = list(dataset.columns[0:32])
plt.matshow(dataset[features].corr(), fignum=f.number, cmap='PiYG_r')
plt.xticks(range(dataset[features].select_dtypes(['number']).shape[1]), dataset[features].select_dtypes(['number']).columns, fontsize=14, rotation=90)
plt.yticks(range(dataset[features].select_dtypes(['number']).shape[1]), dataset[features].select_dtypes(['number']).columns, fontsize=14)
cb = plt.colorbar()
cb.ax.tick_params(labelsize=14)
plt.title('Correlation Matrix', fontsize=16, pad=130);

"""# Listing 7
#### Rescaling Data, Standardize Data, Normalize Data, Binarize Data.
**Analysis**

The unit of measurement for each feature was not provided in the data dictionary documentation. Features such as radius have a different unit and scale than those such as texture or symmetry. Rescaling allows for all values to fall between the given interval, [0,1]. For a given feature, the new value is proportional to the original value's place inside the spread of [0, max(feature)]. For example, the texture_mean of the first value (10.38) lies closer to the original minimum (9.71) than the 25th percentile (16.17). Rescaling transforms the value from 10.38 to 0.023: (10.38 - 9.71) / (39.28 - 9.71) = 0.023

Standardization (Z-score normalization) of the data assumes Gaussian distributions of real-valued features. By standardizing to a mean of 0 and a standard deviation of 1, better performance should be seen with linear algorithms such as logistic regression and linear discriminant analysis. The minimum and maximum values are highly influenced by the presence of outliers, which may increase the overall spread. Minimum and maximum values for each feature vary, dependent on the original spread of each feature.

Normalization does not assume Gaussian distribution, and lends itself to algorithms that don't make this assumption, such as k-nearest neighbors and artificial neural networks. By having the bound at a fixed range, 0 to 1, the smaller standard deviations lead to sensitivity to outliers.

Binarization transforms the values to 0 or 1, dependent on whether each value falls below or above a given threshold. The features of this dataset all have a theoretical minimum of 0, but differing maximums. Given the differences in spread, rather than fix the threshold to a constant, we vary it based on the mean of each feature. This can be achieved by providing a list of the 30 feature means as the binarizer threshold, or by fitting and transforming on a standardized dataset and setting the threshold to the standardized mean of 0.
"""

# Set precision.
np.set_printoptions(precision=3)

# Separate out target variable and print original values for first two rows.
X = dataset.values[:, 1:]
Y = dataset.values[:, 0]
print(X[0:2, :])

"""a) Rescaling"""

scaler_rescaled = MinMaxScaler(feature_range=(0, 1))
X_rescaled = scaler_rescaled.fit_transform(X)
print(X_rescaled[0:2, :])

"""b) Standardization"""

scaler_standardized = StandardScaler()
X_standardized = scaler_standardized.fit_transform(X)
print(X_standardized[0:2, :])

"""c) Normalization"""

scaler_normalized = Normalizer()
X_normalized = scaler_normalized.fit_transform(X)
print(X_normalized[0:2, :])

"""d) Binarization"""

# Print the 30 feature means.
print(X.mean(axis=0))

# Binarize X with the means as thresholds.
binarizer = Binarizer(threshold=X.mean(axis=0)).fit(X)
X_binarized = binarizer.transform(X)
print(X_binarized[0:2, :])

# Alternatively, binarize the standarized X with the threshold at the standard mean of 0.
binarizer = Binarizer(threshold=0).fit(X_standardized)
X_binarized = binarizer.transform(X_standardized)
print(X_binarized[0:2, :])